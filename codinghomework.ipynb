{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "0d008810b9c8467bcb3ca39aa2180e5b81b3a9acb136aab30d47954377cc5120"
   }
  },
  "interpreter": {
   "hash": "eb149be0e47a3f244571d644320a48e5b4552058969aee691b20bf44c81b9cb9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# References:\n",
    "\n",
    "### https://github.com/homefish/edX_Learning_From_Data_2017/blob/master/homework_1/homework_1_PLA.ipynb\n",
    "### https://github.com/homefish/edX_Learning_From_Data_2017/blob/master/homework_2/homework_2_problem_5_6_linear_regression.ipynb\n",
    "### https://github.com/homefish/edX_Learning_From_Data_2017/blob/master/homework_2/homework_2_problem_8_9_10_Nonlinear_Transformation.ipynb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick N data points (x, y) uniformly from the box [-1,1] x [-1,1]\n",
    "N = 100\n",
    "X = np.transpose(np.array([np.ones(N), rnd(N), rnd(N)]))           # input\n",
    "\n",
    "# Classify these points\n",
    "y_f = np.sign(np.dot(X, w_f))\n",
    "\n",
    "\n",
    "# plot points and color them according to their classification\n",
    "plt.plot(X[:,1][y_f == 1], X[:,2][y_f == 1], 'ro')\n",
    "plt.plot(X[:,1][y_f == -1], X[:,2][y_f == -1], 'bo')\n",
    "\n",
    "\n",
    "# plot line\n",
    "# create some data points on the line (for the plot) using the parametric vector form of a line\n",
    "# line(t) = A + t * d,  where A is a point on the line, d the directional vector and t the parameter\n",
    "d = B - A\n",
    "line_x = [A[0] + t * d[0] for t in range(-10,10)]\n",
    "line_y = [A[1] + t * d[1] for t in range(-10,10)]\n",
    "plt.plot(line_x, line_y)\n",
    "\n",
    "# plot the two points that define the line\n",
    "plt.plot(A[0], A[1], 'go')            \n",
    "plt.plot(B[0], B[1], 'go')\n",
    "\n",
    "\n",
    "# set the ranges for the x and y axis to display the [-1,1] x [-1,1] box\n",
    "plt.ylim(-1,1)\n",
    "plt.xlim(-1,1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION\n",
    "X_dagger = np.dot(np.linalg.inv(np.dot(X.T, X)), X.T)\n",
    "w_lr = np.dot(X_dagger, y_f)\n",
    "\n",
    "\n",
    "# plot classification according to w found by linear regression\n",
    "# it shows that some of the points are missclassified\n",
    "y_lr = np.sign(np.dot(X, w_lr))\n",
    "print(\"check dimensions of y_lr: \", y_lr.shape)\n",
    "\n",
    "# plot points and color them according to their classification\n",
    "plt.plot(X[:,1][y_lr == 1], X[:,2][y_lr == 1], 'ro')\n",
    "plt.plot(X[:,1][y_lr == -1], X[:,2][y_lr == -1], 'bo')\n",
    "\n",
    "# plot the correct classification line (target function)\n",
    "plt.plot(line_x, line_y, 'g')\n",
    "plt.ylim(-1,1)\n",
    "plt.xlim(-1,1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problems 5 and 6\n",
    "# START actual HOMEWORK now\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rnd(n): \n",
    "    return np.random.uniform(-1, 1, size = n)\n",
    "\n",
    "\n",
    "# repeat the experiment 1000 times\n",
    "RUNS = 1000\n",
    "N_sample = 100\n",
    "E_in_total = 0\n",
    "E_out_total = 0\n",
    "N_test = 1000\n",
    "\n",
    "for run in range(RUNS):\n",
    "    # choose two random points A, B in [-1,1] x [-1,1]\n",
    "    A = rnd(2)\n",
    "    B = rnd(2)\n",
    "\n",
    "    # the line can be described by y = m*x + b where m is the slope\n",
    "    m = (B[1] - A[1]) / (B[0] - A[0])\n",
    "    b = B[1] - m * B[0]  \n",
    "    w_f = np.array([b, m, -1])\n",
    "\n",
    "    #-----------------------\n",
    "\n",
    "    # Create N data points (x, y) from the target function\n",
    "    X = np.transpose(np.array([np.ones(N_sample), rnd(N_sample), rnd(N_sample)]))           # input\n",
    "    y_f = np.sign(np.dot(X, w_f))\n",
    "    \n",
    "    #-----------------------\n",
    "    \n",
    "    # LINEAR REGRESSION\n",
    "    X_dagger = np.dot(np.linalg.inv(np.dot(X.T, X)), X.T)\n",
    "    w_lr = np.dot(X_dagger, y_f)\n",
    "    \n",
    "    # classification according to w found by linear regression\n",
    "    y_lr = np.sign(np.dot(X, w_lr))\n",
    "    \n",
    "    #------------------------\n",
    "    \n",
    "    # Error E_in\n",
    "    E_in = sum(y_lr != y_f) / N_sample\n",
    "    E_in_total += E_in\n",
    "\n",
    "    #------------------------\n",
    "    # Problem 6: Take 1000 test points (out of sample points) and count disagreement\n",
    "    # between y_f_test and y_lr_test\n",
    "    X_test = np.transpose(np.array([np.ones(N_test), rnd(N_test), rnd(N_test)]))\n",
    "    y_f_test = np.sign(np.dot(X_test, w_f))\n",
    "    y_lr_test = np.sign(np.dot(X_test, w_lr))\n",
    "    \n",
    "    E_out = sum(y_lr_test != y_f_test) / N_test\n",
    "    E_out_total += E_out\n",
    "    \n",
    "    \n",
    "# Average of E_in over RUNS\n",
    "E_in_avg = E_in_total / RUNS\n",
    "print(\"Average of E_in over\", RUNS, \" runs:\", E_in_avg)\n",
    "\n",
    "# Average of E_out over RUNS\n",
    "E_out_avg = E_out_total / RUNS\n",
    "print(\"Average of E_out over\", RUNS, \" runs:\", E_out_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nonlinear Transformation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create 1000 random points\n",
    "N_train = 1000\n",
    "\n",
    "def rnd(n):\n",
    "    return np.random.uniform(-1, 1, size = n)\n",
    "\n",
    "# matrix consisting of feature vectors\n",
    "X_train = np.transpose(np.array([np.ones(N_train), rnd(N_train), rnd(N_train)]))\n",
    "y_f_train = np.sign(np.multiply(X_train[:,1], X_train[:,1]) + np.multiply(X_train[:,2], X_train[:,2]) - 0.6)\n",
    "print(X_train.shape)\n",
    "print(y_f_train.shape)\n",
    "\n",
    "\n",
    "# pick 10% = 100 random indices\n",
    "indices = list(range(N_train))\n",
    "np.random.shuffle(indices)\n",
    "random_indices = indices[:(N_train // 10)]\n",
    "\n",
    "\n",
    "# flip sign in y_f_train vector\n",
    "for idx in random_indices:\n",
    "    y_f_train[idx] = (-1) * y_f_train[idx]\n",
    "\n",
    "# linear regression\n",
    "X_dagger = np.dot(np.linalg.inv(np.dot(X_train.T, X_train)), X_train.T)\n",
    "w_lr_train = np.dot(X_dagger, y_f_train)\n",
    "\n",
    "# calculate E_in\n",
    "y_lr_train = np.sign(np.dot(X_train, w_lr_train))\n",
    "E_in = sum(y_lr_train != y_f_train)  / N_train\n",
    "print(\"In sample error: \", E_in)\n",
    "\n",
    "\n",
    "# Create a plot of the classified points\n",
    "plt.plot(X_train[:,1][y_f_train == 1], X_train[:,2][y_f_train == 1], 'ro')\n",
    "plt.plot(X_train[:,1][y_f_train == -1], X_train[:,2][y_f_train == -1], 'bo')\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 8\n",
    "# Now do this 1000 times to take average\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rnd(n):\n",
    "    return np.random.uniform(-1, 1, size = n)\n",
    "\n",
    "\n",
    "RUNS = 1000\n",
    "N_train = 1000\n",
    "E_in_total = 0\n",
    "\n",
    "for run in range(RUNS):\n",
    "    \n",
    "    # create 1000 random points\n",
    "    # matrix consisting of feature vectors\n",
    "    X_train = np.transpose(np.array([np.ones(N_train), rnd(N_train), rnd(N_train)]))\n",
    "    y_f_train = np.sign(X_train[:,1] * X_train[:,1] + X_train[:,2] * X_train[:,2] - 0.6)\n",
    "\n",
    "    # pick 10% = 100 random indices\n",
    "    indices = list(range(N_train))\n",
    "    np.random.shuffle(indices)\n",
    "    random_indices = indices[:(N_train // 10)]\n",
    "\n",
    "    # flip sign in y_f_train vector\n",
    "    for idx in random_indices:\n",
    "        y_f_train[idx] = (-1) * y_f_train[idx]\n",
    "\n",
    "    # linear regression\n",
    "    X_dagger = np.dot(np.linalg.inv(np.dot(X_train.T, X_train)), X_train.T)\n",
    "    w_lr_train = np.dot(X_dagger, y_f_train)\n",
    "\n",
    "    # calculate E_in\n",
    "    y_lr_train = np.sign(np.dot(X_train, w_lr_train))\n",
    "    E_in = sum((y_lr_train != y_f_train))  / N_train\n",
    "    E_in_total += E_in\n",
    "    #print(\"In sample error: \", E_in)\n",
    "\n",
    "    \n",
    "E_in_avg = E_in_total / RUNS\n",
    "print(\"The average error E_in over\", RUNS, \"runs is: E_in_avg = \", E_in_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 9 :  transform the N = 1000 training data into the following nonlinear feature\n",
    "# vector: (1, x1, x2, x1*x2, x1*x1, x2*x2)\n",
    "\n",
    "# How to concatenate extra columns to X ?\n",
    "X = X_train\n",
    "\n",
    "# new feature matrix\n",
    "X_trans = np.transpose(np.array([np.ones(N_train), X[:,1], X[:,2], X[:,1]*X[:,2], X[:,1]*X[:,1], X[:,2]*X[:,2]]))\n",
    "\n",
    "\n",
    "# linear regression on the new \"feature matrix\"\n",
    "X_dagger_trans = np.dot(np.linalg.inv(np.dot(X_trans.T, X_trans)), X_trans.T)\n",
    "w_lr_trans = np.dot(X_dagger_trans, y_f_train)\n",
    "\n",
    "# try the different hypotheses that are given\n",
    "w_a = np.array([-1, -0.05, 0.08, 0.13, 1.5, 1.5])\n",
    "w_b = np.array([-1, -0.05, 0.08, 0.13, 1.5, 15])\n",
    "w_c = np.array([-1, -0.05, 0.08, 0.13, 15, 1.5])\n",
    "w_d = np.array([-1, -1.5, 0.08, 0.13, 0.05, 0.05])\n",
    "w_e = np.array([-1, -0.05, 0.08, 1.5, 0.15, 0.15])\n",
    "\n",
    "# compute classifications made by each hypothesis\n",
    "y_lr_trans = np.sign(np.dot(X_trans, w_lr_trans))\n",
    "y_a = np.sign(np.dot(X_trans, w_a))\n",
    "y_b = np.sign(np.dot(X_trans, w_b))\n",
    "y_c = np.sign(np.dot(X_trans, w_c))\n",
    "y_d = np.sign(np.dot(X_trans, w_d))\n",
    "y_e = np.sign(np.dot(X_trans, w_e))\n",
    "\n",
    "mismatch_lr_and_a = sum(y_a != y_lr_trans) / N_train                 # ALWAYS RESTART KERNEL !!!!!!!!!!!!                                                         \n",
    "mismatch_lr_and_b = sum(y_b != y_lr_trans) / N_train\n",
    "mismatch_lr_and_c = sum(y_c != y_lr_trans) / N_train\n",
    "mismatch_lr_and_d = sum(y_d != y_lr_trans) / N_train\n",
    "mismatch_lr_and_e = sum(y_e != y_lr_trans) / N_train\n",
    "\n",
    "print(\"mismatch between LR and a = \", mismatch_lr_and_a)\n",
    "print(\"mismatch between LR and b = \", mismatch_lr_and_b)\n",
    "print(\"mismatch between LR and c = \", mismatch_lr_and_c)\n",
    "print(\"mismatch between LR and d = \", mismatch_lr_and_d)\n",
    "print(\"mismatch between LR and e = \", mismatch_lr_and_e)\n",
    "\n",
    "print(\"The weight vector of my hypothesis is: w_LR = \", w_lr_trans)\n",
    "# Use that weight vector for problem 10\n",
    "\n",
    "\n",
    "# compare predictions made by w_lr_trans with those made by targer function\n",
    "print(\"Sanity check: E_in = \", sum(y_f_train != y_lr_trans) / N_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 10\n",
    "\n",
    "RUNS = 1000\n",
    "N_test = 1000\n",
    "E_out_total = 0\n",
    "\n",
    "for run in range(RUNS):\n",
    "    \n",
    "    # create 1000 random points\n",
    "    # matrix consisting of feature vectors\n",
    "    X_test = np.transpose(np.array([np.ones(N_train), rnd(N_train), rnd(N_train)]))\n",
    "    y_f_test = np.sign(X_test[:,1] * X_test[:,1] + X_test[:,2] * X_test[:,2] - 0.6)\n",
    "\n",
    "    # pick 10% = 100 random indices\n",
    "    indices = list(range(N_test))\n",
    "    np.random.shuffle(indices)\n",
    "    random_indices = indices[:(N_test // 10)]\n",
    "\n",
    "    # flip sign in y_f_train vector\n",
    "    for idx in random_indices:\n",
    "        y_f_test[idx] = (-1) * y_f_test[idx]\n",
    "\n",
    "    # Compute classification made by my hypothesis from Problem 9\n",
    "    # first create transformed feature matrix\n",
    "    X = X_test\n",
    "    X_trans_test = np.transpose(np.array([np.ones(N_test), X[:,1], X[:,2], X[:,1]*X[:,2], X[:,1]*X[:,1], X[:,2]*X[:,2]]))\n",
    "    y_lr_trans_test = np.sign(np.dot(X_trans_test, w_lr_trans))\n",
    "    \n",
    "    # Compute disagreement between hypothesis and target function\n",
    "    E_out = sum(y_lr_trans_test != y_f_test) / N_train\n",
    "    E_out_total += E_out\n",
    "    \n",
    "E_out_avg = E_out_total / RUNS\n",
    "print(\"The average error E_out over\", RUNS, \"runs is: E_out_avg = \", E_out_avg)"
   ]
  }
 ]
}